### 梯度下降拟合y=kx+b
```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import train_test_split

def get_samples():
    x = np.linspace(-1, 1, 100)
    y = 3.14159 * x + 1.0 + np.random.normal(0, 0.1, size=x.shape[0])
    return x, y

def gradient_descent(feature, values):
    n = feature.shape[0]
    k = np.random.normal(0, 1)
    b = np.random.normal(0, 1)
    learning_rate = 0.1

    print('initial parameter : ', k, b)
    for iteration in range(20):
        sum_loss = 0.0
        for i in range(n):
            ty = k * feature[i] + b
            delta = ty - values[i]
            grad_k = delta * feature[i]
            grak_b = delta
            k = k - learning_rate * grad_k
            b = b - learning_rate * grak_b
            temp = k * feature[i] + b - values[i]
            sum_loss += np.sqrt(temp * temp)
        print('iteration : ', iteration+1, k, b, sum_loss)
    print('final parameter : ', k, b)


if __name__ == '__main__':
    x, y = get_samples()
    gradient_descent(x, y)

```
