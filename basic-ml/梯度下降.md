### 梯度下降拟合y=kx+b
```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import train_test_split

def get_samples():
    x = np.linspace(-1, 1, 100)
    y = 3.14159 * x + 1.0 + np.random.normal(0, 0.1, size=x.shape[0])
    return x, y

def gradient_descent(feature, values):
    n = feature.shape[0]
    k = np.random.normal(0, 1)
    b = np.random.normal(0, 1)
    learning_rate = 0.1

    print('initial parameter : ', k, b)
    for iteration in range(20):
        sum_loss = 0.0
        for i in range(n):
            ty = k * feature[i] + b
            delta = ty - values[i]
            grad_k = delta * feature[i]
            grak_b = delta
            k = k - learning_rate * grad_k
            b = b - learning_rate * grak_b
            temp = k * feature[i] + b - values[i]
            sum_loss += np.sqrt(temp * temp)
        print('iteration : ', iteration+1, k, b, sum_loss)
    print('final parameter : ', k, b)


if __name__ == '__main__':
    x, y = get_samples()
    gradient_descent(x, y)

```

### 梯度下降拟合 f(x, y) = 6.18 * x + 3.82 * y + 0.5
```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import train_test_split

def get_samples():
    xx = np.linspace(-1, 1, 50)
    yy = np.linspace(-1, 1, 50)
    X, Y = np.meshgrid(xx, yy)
    theta = np.zeros(2)
    theta[0] = 6.18
    theta[1] = 3.82
    b = 0.5
    Z = X * theta[0] + Y * theta[1] + b + np.random.normal(0, 0.02, size=X.shape)
    return X, Y, Z


def gradient_descent(feature, values):
    theta = np.random.normal(0, 1, size=feature.shape[1])
    b = np.random.normal()

    learning_rate = 0.1
    print('initial param: ', theta, b)
    n = feature.shape[0]
    for iteration in range(1):
        sum_loss = 0.0
        for i in range(n):
            ty = np.dot(theta, feature[i]) + b
            delta = ty - values[i]
            grad_theta = delta * feature[i]
            grad_b = delta
            theta = theta - learning_rate * grad_theta
            b = b - learning_rate * grad_b
            sum_loss = np.sqrt(delta * delta)
            if i % 100 == 0:
                print('iteration : ', i, theta, b, sum_loss)


if __name__ == '__main__':
    X, Y, Z = get_samples()
    feature = np.array([X.flatten(), Y.flatten()]).T
    values = Z.flatten()
    X_train, X_test, y_train, y_test = train_test_split(feature, values)
    gradient_descent(X_train, y_train)
```
