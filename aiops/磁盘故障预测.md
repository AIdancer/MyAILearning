### backblaze数据集
    https://www.backblaze.com/b2/hard-drive-test-data.html
    数据集包含了常用厂家磁盘的各种型号及SMART特征（包括归一化后的特征及原始特征）
    
### 分类方法对比


### 特征提取
```python
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd

def get_all_files(root_dir):
    ret = []
    for root, dirs, files in os.walk(root_dir):
        for file in files:
            fname = r'{}\{}'.format(root, file)
            ret.append(fname)
    return ret

files = get_all_files(r'D:\afirm\data\backblaze')

models = set()

for fname in files:
    print('processing file : {}'.format(fname))
    df = pd.read_csv(fname)
    models = models | set(df['model'])
    
def check_disks_count(model_name):
    files = get_all_files(r'D:\afirm\data\backblaze')
    count = 0
    for fname in files:
        print('processing file : {}'.format(fname))
        df = pd.read_csv(fname)
        df = df.loc[df['model'] == model_name]
        count += df.shape[0]
    print('{}  :   {}'.format(model_name, count))
    return count

total = check_disks_count('Seagate BarraCuda SSD ZA2000CM10002')

files = get_all_files(r'D:\afirm\data\backblaze')
model_name = ''
frame_list =[]
for fname in files:
    print('processing file : {}'.format(fname))
    tmp_df = pd.read_csv(fname)
    tmp_df = tmp_df.loc[tmp_df['model'] == 'Seagate BarraCuda SSD ZA250CM10002']
    tmp_df.reset_index(drop=True, inplace=True)
    
    frame_list.append(tmp_df)
df = pd.concat(frame_list)
print(df.shape)
df.head(5)
df.fillna(value=0, inplace=True)

sn_set = set(df['serial_number'])
print(len(sn_set))
df.to_csv('data.csv')


```

### 模型分类
```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import xgboost

from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from xgboost import XGBClassifier

def show_detail(pred, y):
    n = pred.shape[0]
    tp, fp, tn, fn = 0, 0, 0, 0
    for i in range(n):
        if pred[i] == 1:
            if y[i] == 1:
                tp += 1
            else:
                fp += 1
        else:
            if y[i] == 0:
                tn += 1
            else:
                fn += 1
    print('准确率 : {:.5f}%'.format((tp + tn) * 100.0 / (tp + fp + tn + fn)))
    print('召回率 : {:.5f}%'.format(tp * 100.0 / (tp + fn)))
    print('误报率 : {:.5f}%'.format(fp * 100.0 / (fp + tn)))
    print('- - - - - - - - - - - - - - - - - - - - - - - -')
    
X = np.load('X.npy', allow_pickle=True)
y = np.load('y.npy', allow_pickle=True)

n = X.shape[0]
print(n)

features = []
labels = []

for i in range(n):
    if y [i] == 0:
        if np.random.rand() < 0.05:
            features.append(X[i])
            labels.append(y[i])
    elif np.random.rand() <= 0.5:
        for j in range(10):
            features.append(X[i])
            labels.append(y[i])
features = np.vstack(features)
labels = np.array(labels)

features = np.array(features, dtype=np.float32)

print(features.shape)
print(labels.shape)
print(np.sum(labels))

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
print(max(y_train), min(y_train))
print(y_train)

param_dist = {'objective':'multi:softmax', 'n_estimators':200, 'max_depth':6, 'use_label_encoder':False, 'num_class' : 2}
xgb_model = XGBClassifier(**param_dist)
xgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train)], eval_metric='mlogloss', verbose=True)

pred = xgb_model.predict(X_train)
pred_test = xgb_model.predict(X_test)

pred_all = xgb_model.predict(X)

print('train score : ', accuracy_score(pred, y_train))
show_detail(pred, y_train)
print('test score : ', accuracy_score(pred_test, y_test))
show_detail(pred_test, y_test)
print('all score : ', accuracy_score(pred_all, y))
show_detail(pred_all, y)

class MyDataSet(Dataset):
    def __init__(self, _x, _y):
        xx, yy = np.array(_x, dtype=np.float32), np.array(_y)
        self.X = torch.tensor(xx, dtype=torch.float32)
        self.y = torch.tensor(yy, dtype=torch.long)
    
    def __getitem__(self, index):
        return self.X[index], self.y[index]
    
    def __len__(self):
        return self.y.shape[0]
        
class ResNet(nn.Module):
    def __init__(self, input_size):
        super(ResNet, self).__init__()
        self.residual = nn.Linear(input_size, input_size)
        
    def forward(self, x):
        out = F.relu(self.residual(x))
        return x + out
    
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.linear1 = nn.Linear(216, 1024)
        self.resnet = ResNet(1024)
        self.linear2 = nn.Linear(1024, 2)

    def forward(self, x):
        out = F.relu(self.linear1(x))
        encode = self.resnet(out)
        decode = self.linear2(out)
        return encode, decode
        

net = Net()
for p in net.parameters():
    nn.init.normal_(p, mean=0, std=0.02)

loss = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), lr = 0.001)

batch_size = 64
# loader = DataLoader(MyDataSet(X, y), batch_size=batch_size)
loader = DataLoader(MyDataSet(X_train, y_train), batch_size=batch_size)
n = y.shape[0]
for epoch in range(700):
    sum_loss = 0.0
    for xi, yi in loader:
        encode, decode = net(xi)
        l = loss(decode, yi)
        optimizer.zero_grad()
        l.backward()
        optimizer.step()
        sum_loss += l.item()
    print('epoch : {}   loss:{:.5f}'.format(epoch + 1, sum_loss))
    
encode, out = net(torch.tensor(np.array(X_train, dtype=np.float32), dtype=torch.float32))
pred_prob = F.softmax(out, dim=1)
total, correct = 0, 0
label = y_train
indexs = torch.argmax(out, dim=1)
for i in range(label.shape[0]):
    total += 1
    c = indexs[i].item()
    if c == int(label[i]):
        correct += 1
print('训练集 acc : %.2f%%' % (correct * 100.0 / total))
show_detail(indexs.numpy(), y_train)
# 训练集 acc : 99.94%

encode, out = net(torch.tensor(np.array(X_test, dtype=np.float32), dtype=torch.float32))
pred_prob = F.softmax(out, dim=1)
total, correct = 0, 0
label = y_test
indexs = torch.argmax(out, dim=1)
for i in range(label.shape[0]):
    total += 1
    c = indexs[i].item()
    if c == int(label[i]):
        correct += 1
print('测试 acc : %.2f%%' % (correct * 100.0 / total))
show_detail(indexs.numpy(), y_train)

encode, out = net(torch.tensor(np.array(X, dtype=np.float32), dtype=torch.float32))
pred_prob = F.softmax(out, dim=1)
total, correct = 0, 0
label = y
indexs = torch.argmax(out, dim=1)
for i in range(label.shape[0]):
    total += 1
    c = indexs[i].item()
    if c == int(label[i]):
        correct += 1
print('全集 acc : %.2f%%' % (correct * 100.0 / total))
show_detail(indexs.numpy(), y)

features = np.array(features, dtype=np.float32)
labels = np.array(labels)
print(features.dtype)
print(labels.dtype)

print(features.shape)
print(labels.shape)
x_new, _ = net(torch.tensor(features, dtype=torch.float32))
y_new = np.array(labels, dtype=np.int32)
x_new = x_new.detach().numpy()

print(x_new.shape)
X_train, X_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.3, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

param_dist = {'objective':'multi:softmax', 'n_estimators':200, 'max_depth':6, 'use_label_encoder':False, 'num_class' : 2}
xgb_model = XGBClassifier(**param_dist)

xgb_model.fit(x_new, y_new, eval_set=[(x_new, y_new)], eval_metric='mlogloss', verbose=True)

x_all, _ = net(torch.tensor(np.array(X, dtype=np.float32), dtype=torch.float32))
y_all = np.array(y, dtype=np.int32)
x_all = x_all.detach().numpy()

pred = xgb_model.predict(X_train)
pred_test = xgb_model.predict(X_test)
pred_all = xgb_model.predict(x_all)

print('train score : {:.5f}'.format(accuracy_score(pred, y_train)))
show_detail(pred, y_train)
print('test score : {:.5f}'.format(accuracy_score(pred_test, y_test)))
show_detail(pred_test, y_test)
print('all score : {:.5f}'.format(accuracy_score(pred_all, y_all)))
show_detail(pred_all, y_all)

```
